---
layout: post
title: luceneè¯æ¡åˆ†æ
tags:
- lucene
categories: lucene
description: analysis
---

lucene analysis å°±æ˜¯ä¸€ä¸ªè§£å†³å¦‚ä½•å°†ä¸€ä¸ªé•¿å¥è§£ææˆå¤šä¸ªè¯æ¡çš„å·¥å…·åŒ…ã€‚

# æ ¸å¿ƒç±»å›¾

<img src="/assets/img/lucene/analysisAnalysis.svg" />

- CharTermAttribute, è®°å½•tokençš„è¯æ¡æ–‡æœ¬ä¿¡æ¯ã€‚
- OffsetAttribute, è®°å½•tokençš„åç§»é‡ä½ç½®ä¿¡æ¯ã€‚
- PositionLengthAttribute, è®°å½•tokençš„å ä½å®½åº¦ä¿¡æ¯ï¼Œé»˜è®¤å€¼ä¸º1ï¼Œåªåœ¨å›¾ç»“æ„(graph)ä¸­ä½œç”¨è¾ƒå¤§ã€‚
- PositionIncrementAttribute, è®°å½•tokenç›¸å¯¹äºä¸Šä¸€ä¸ªtokençš„ä½ç½®å¢é•¿é‡ä¿¡æ¯ï¼Œé»˜è®¤å€¼ä¸º1ï¼Œåœ¨å¤šè¯å¹²(stem)å•è¯æœç´¢å’Œå¿½ç•¥åœæ­¢è¯çŸ­è¯­åŒ¹é…çš„
  åœºæ™¯ä¸‹æœ‰å‘æŒ¥ä½œç”¨ã€‚
 


# ä½¿ç”¨TokenStream API

- åˆ›å»ºTokenStreamå¯¹è±¡ã€‚
- è°ƒç”¨streamçš„resetæ–¹æ³•ã€‚
- æ£€ç´¢streamå†…éƒ¨æ‰€æœ‰çš„Attributeã€‚
- ä¸€ç›´è°ƒç”¨incrementTokenç›´åˆ°è¿”å›falseã€‚
- è°ƒç”¨streamçš„endæ–¹æ³•ã€‚
- è°ƒç”¨streamçš„closeæ–¹æ³•ã€‚
  
```java
public class MyAnalyzer extends Analyzer {

    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
        WhitespaceTokenizer whitespaceTokenizer = new WhitespaceTokenizer(); // 1
        return new TokenStreamComponents(whitespaceTokenizer); // å†…éƒ¨ä¼šè°ƒç”¨ whitespaceTokenizer.setReaderæ–¹æ³•
    }

    public static void main(String[] args) throws IOException {
        // text to tokenize
        final String text = " This is a demo of the TokenStream ğŸ˜Š API";

        MyAnalyzer analyzer = new MyAnalyzer();
        // tokenStreamä¼šå›è°ƒä¸Šé¢çš„createComponentsæ–¹æ³•
        // è¿™é‡Œè¿”å›çš„streamå°±æ˜¯ä¸Šé¢æ ‡è®°1å¤„çš„WhitespaceTokenizer
        TokenStream stream = analyzer.tokenStream("field", new StringReader(text));

        // get the CharTermAttribute from the TokenStream
        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);

        try {
            stream.reset();

            // print all tokens until stream is exhausted
            while (stream.incrementToken()) {
                // This
                //is
                //a
                //demo
                //of
                //the
                //TokenStream
                //ğŸ˜Š
                //API
                System.out.println(termAtt.toString());
            }

            stream.end();
        } finally {
            stream.close();
        }
    }
}
```
æºç è§[è¿™é‡Œ](https://github.com/gskeno/lucene/blob/branch_9_7_notes/lucene/analysis/common/src/test/org/apache/lucene/analysis/MyAnalyzer.java)

Analyzerè´Ÿè´£åˆ›å»ºåˆ†è¯ç»„ä»¶`Tokenizer` å’Œ `TokenFilter`ï¼ŒTokenizerä¸»è¦ç”¨æ¥å¯¹åŸå§‹å¥å­è¿›è¡Œåˆ†è¯è·å¾—tokenï¼ŒTokenFilterçš„å®ç°ä½¿ç”¨äº†`åŒ…è£…è€…è®¾è®¡æ¨¡å¼`ï¼Œä¸»è¦ç”¨æ¥å¯¹tokenè¿›è¡Œè¿‡æ»¤æˆ–æ›´æ”¹ï¼ŒäºŒè€…éƒ½ç»§æ‰¿è‡ª`TokenStream`ã€‚

## tokené•¿åº¦è¿‡æ»¤
```java
public class MyLengthAnalyzer extends Analyzer {
    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
        WhitespaceTokenizer whitespaceTokenizer = new WhitespaceTokenizer();
        // åŒ…è£…è€…è®¾è®¡æ¨¡å¼
        // ä¼šè¿‡æ»¤æ‰é•¿åº¦<3çš„token
        LengthFilter lengthFilter = new LengthFilter(whitespaceTokenizer, 3, Integer.MAX_VALUE);
        TokenStreamComponents tokenStreamComponents = new TokenStreamComponents(whitespaceTokenizer, lengthFilter);
        return tokenStreamComponents;
    }

    public static void main(String[] args) throws IOException {
        // text to tokenize, é•¿åº¦å°äº3çš„è¯æ¡ä¸ä¼šè¢«è¾“å‡º
        final String text = "This is a demo of the TokenStream API";

        MyLengthAnalyzer analyzer = new MyLengthAnalyzer();
        TokenStream stream = analyzer.tokenStream("field", new StringReader(text));

        // get the CharTermAttribute from the TokenStream
        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);

        try {
            stream.reset();

            // print all tokens until stream is exhausted è¿‡æ»¤æ‰é•¿åº¦å°äº3çš„token
            // This
            // demo
            // the
            // TokenStream
            // API
            while (stream.incrementToken()) {
                System.out.println(termAtt.toString());
            }

            stream.end();
        } finally {
            stream.close();
        }
    }
}
```

## è‡ªå®šä¹‰Attribute
å¯¹äºæ¯ä¸ªtokenè€Œè¨€ï¼Œæˆ‘ä»¬å‡è®¾å…¶é¦–å­—æ¯ä¸ºå¤§å†™æ—¶æ˜¯åè¯ï¼Œå…¶ä»–æƒ…å†µä¸º"Unknown"ï¼Œè¦åœ¨åˆ†è¯å™¨ä¸­ä½¿ç”¨ï¼Œéœ€è¦ä»¥ä¸‹å‡ æ­¥ã€‚

- è‡ªå®šä¹‰Attributeå­æ¥å£ï¼Œå¹¶ç¼–å†™å®ç°å­ç±»ã€‚
- ç¼–å†™ä¸€ä¸ªTokenFilterè¿‡æ»¤å™¨ï¼Œå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œé‰´åˆ«å…¶è¯æ€§(æ˜¯å¦æ˜¯åè¯)
  
```java
public interface PartOfSpeechAttribute extends Attribute {
    enum PartOfSpeech {
        Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Article, Unknown
    }

    public void setPartOfSpeech(PartOfSpeech pos);

    public PartOfSpeech getPartOfSpeech();
}

public final class PartOfSpeechAttributeImpl extends AttributeImpl
        implements PartOfSpeechAttribute {

    private PartOfSpeech pos = PartOfSpeech.Unknown;

    public void setPartOfSpeech(PartOfSpeech pos) {
        this.pos = pos;
    }

    public PartOfSpeech getPartOfSpeech() {
        return pos;
    }

    @Override
    public void clear() {
        pos = PartOfSpeech.Unknown;
    }

    @Override
    public void reflectWith(AttributeReflector reflector) {

    }

    @Override
    public void copyTo(AttributeImpl target) {
        ((PartOfSpeechAttribute) target).setPartOfSpeech(pos);
    }

}


class PartOfSpeechTaggingFilter extends TokenFilter {
    PartOfSpeechAttribute posAtt = addAttribute(PartOfSpeechAttribute.class);
    CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);

    protected PartOfSpeechTaggingFilter(TokenStream input) {
        super(input);
    }

    public boolean incrementToken() throws IOException {
        if (!input.incrementToken()) {return false;}
        posAtt.setPartOfSpeech(determinePOS(termAtt.buffer(), 0, termAtt.length()));
        return true;
    }

    // determine the part of speech for the given term
    protected PartOfSpeechAttribute.PartOfSpeech determinePOS(char[] term, int offset, int length) {
        // naive implementation that tags every uppercased word as noun
        if (length > 0 && Character.isUpperCase(term[0])) {
            return PartOfSpeechAttribute.PartOfSpeech.Noun;
        }
        return PartOfSpeechAttribute.PartOfSpeech.Unknown;
    }
}

public class PartOfSpeechAnalyzer extends Analyzer {

    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
        final Tokenizer source = new WhitespaceTokenizer();
        TokenStream result = new LengthFilter( source, 3, Integer.MAX_VALUE);
        result = new PartOfSpeechTaggingFilter(result);
        return new TokenStreamComponents(source, result);
    }

    public static void main(String[] args) throws IOException {
        // text to tokenize
        final String text = "This is a demo of the TokenStream API";

        PartOfSpeechAnalyzer analyzer = new PartOfSpeechAnalyzer();
        TokenStream stream = analyzer.tokenStream("field", new StringReader(text));

        // get the CharTermAttribute from the TokenStream
        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);

        // get the PartOfSpeechAttribute from the TokenStream
        PartOfSpeechAttribute posAtt = stream.addAttribute(PartOfSpeechAttribute.class);

        try {
            stream.reset();

            // print all tokens until stream is exhausted
            while (stream.incrementToken()) {
                System.out.println(termAtt.toString() + ": " + posAtt.getPartOfSpeech());
            }

            stream.end();
        } finally {
            stream.close();
        }
    }
}

```
è¾“å‡ºç»“æœ
```text
This: Noun
demo: Unknown
the: Unknown
TokenStream: Noun
API: Noun
```
æºç åœ¨[è¿™é‡Œ](https://github.com/gskeno/lucene/blob/branch_9_7_notes/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PartOfSpeechAnalyzer.java)

# å‚è€ƒ
- [lucene 9.7 analysis](https://lucene.apache.org/core/9_7_0/core/org/apache/lucene/analysis/package-summary.html)